# KitREC Experimental_test ìƒì„¸ ì‘ì—… ê³„íšì„œ

**ì‘ì„±ì¼:** 2025-12-06
**ìµœì¢… ìˆ˜ì •:** 2025-12-07
**ëª©ì :** KitREC ëª¨ë¸ í‰ê°€ ë° Baseline ë¹„êµ ì‹¤í—˜ì„ ìœ„í•œ êµ¬í˜„ ëª…ì„¸ + ì‹¤í—˜ ë¡œë“œë§µ
**ì°¸ì¡° ë¬¸ì„œ:** CLAUDE.md

---

## ğŸ¯ Implementation Status Summary

### âœ… ì½”ë“œ êµ¬í˜„ ì™„ë£Œ (2025-12-07)

| Phase | ìƒíƒœ | ì£¼ìš” ë‚´ìš© |
|-------|------|----------|
| **Phase 1: Critical** | âœ… ì™„ë£Œ | Confidence 1-10 ë²”ìœ„, Train/Test ë¶„ë¦¬, User Type Mapping |
| **Phase 2: High** | âœ… ì™„ë£Œ | Device ìˆ˜ì •, Gradient Clipping, Statistical Testing |
| **Phase 3: Medium** | âœ… ì™„ë£Œ | LLM4CDR Target History, LR Scheduler, Holm Correction |
| **Phase 4: Explain** | âœ… ì™„ë£Œ | GPT-4.1 Rationale Evaluation, MAE/RMSE, Perplexity |
| **Bug Fixes** | âœ… ì™„ë£Œ | ë¬´í•œë£¨í”„ ë°©ì§€, Path validation, Empty list ì²˜ë¦¬ |
| **Phase 6: Refinements** | âœ… ì™„ë£Œ | LLM4CDR BaseEvaluator, per-sample metrics, Logging |

### âœ… ì¶”ê°€ ê°œì„  (2025-12-07 Updated)

| Issue ID | íŒŒì¼ | ìˆ˜ì • ë‚´ìš© | ëª©ì  |
|----------|------|----------|------|
| **A-1** | `llm4cdr/evaluator.py` | BaseEvaluator ìƒì†, Candidate ê²€ì¦, Confidence ì •ê·œí™” | ê³µí†µ ì¸í”„ë¼ í™œìš© |
| **A-2** | `baselines/*/evaluator.py` | per-sample metrics ìˆ˜ì§‘ | t-test í†µê³„ ê²€ì • ì§€ì› |
| **A-3** | `base_evaluator.py` | ê²€ì¦ ì‹¤íŒ¨ ì‹œ logging ì¶”ê°€ | ëŠ¥ë™ì  ì˜¤ë¥˜ íƒì§€ |

---

## ğŸ“Š íŒŒì¼ êµ¬í˜„ í˜„í™©

### âœ… êµ¬í˜„ ì™„ë£Œ íŒŒì¼

| ì¹´í…Œê³ ë¦¬ | íŒŒì¼ | ìƒíƒœ |
|---------|------|------|
| **src/data/** | `data_loader.py`, `prompt_builder.py`, `candidate_handler.py` | âœ… |
| **src/inference/** | `vllm_inference.py`, `output_parser.py`, `batch_inference.py` | âœ… |
| **src/metrics/** | `ranking_metrics.py`, `explainability_metrics.py`, `stratified_analysis.py`, `statistical_analysis.py` | âœ… |
| **src/models/** | `kitrec_model.py`, `base_model.py` | âœ… |
| **src/utils/** | `logger.py`, `io_utils.py`, `visualization.py` | âœ… |
| **baselines/conet/** | `model.py`, `trainer.py`, `evaluator.py`, `data_converter.py` | âœ… |
| **baselines/dtcdr/** | `model.py`, `trainer.py`, `evaluator.py`, `data_converter.py` | âœ… |
| **baselines/llm4cdr/** | `prompts.py`, `evaluator.py` | âœ… |
| **baselines/** | `base_evaluator.py` | âœ… |
| **scripts/** | `run_kitrec_eval.py`, `run_ablation_study.py`, `run_baseline_eval.py` | âœ… |
| **scripts/** | `verify_environment.py`, `verify_env_and_data.py` | âœ… |

### âŒ ë¯¸êµ¬í˜„ íŒŒì¼ (ì„ íƒì‚¬í•­)

| ì¹´í…Œê³ ë¦¬ | íŒŒì¼ | í•„ìš”ì„± |
|---------|------|--------|
| **configs/** | `eval_config.yaml`, `model_paths.yaml`, `baseline_config.yaml` | ì„ íƒ (í•˜ë“œì½”ë”© ëŒ€ì²´ ê°€ëŠ¥) |
| **src/models/** | `conet_wrapper.py`, `dtcdr_wrapper.py`, `llm4cdr_wrapper.py` | ì„ íƒ (ì§ì ‘ í˜¸ì¶œ ê°€ëŠ¥) |
| **scripts/** | `run_stratified_analysis.py`, `run_metadata_subgroup.py`, `generate_report.py` | ì„ íƒ (ìˆ˜ë™ ë¶„ì„ ê°€ëŠ¥) |

---

## ğŸ§ª ì‹¤í—˜ ì‹¤í–‰ í˜„í™©

| ì‹¤í—˜ Phase | ìƒíƒœ | ì„¤ëª… |
|------------|------|------|
| **Phase 1: í™˜ê²½ ì„¤ì •** | â³ ëŒ€ê¸° | vLLM, GPU, HF Token ì„¤ì • í•„ìš” |
| **Phase 2: KitREC í‰ê°€** | âŒ ë¯¸ì‹¤í–‰ | 8ê°œ ëª¨ë¸ Ã— 30,000 ìƒ˜í”Œ |
| **Phase 3: RQ1 Ablation** | âŒ ë¯¸ì‹¤í–‰ | 2Ã—2 êµì°¨ ê²€ì¦ |
| **Phase 4: Baseline í‰ê°€** | âŒ ë¯¸ì‹¤í–‰ | CoNet, DTCDR, LLM4CDR í•™ìŠµ/í‰ê°€ |
| **Phase 5: Stratified ë¶„ì„** | âŒ ë¯¸ì‹¤í–‰ | User Typeë³„, Metadataë³„ |
| **Phase 6: ë¦¬í¬íŠ¸ ìƒì„±** | âŒ ë¯¸ì‹¤í–‰ | ìµœì¢… ë…¼ë¬¸ìš© í…Œì´ë¸”/ê·¸ë˜í”„ |

---

### êµ¬í˜„ëœ ë² ì´ìŠ¤ë¼ì¸

| Baseline | ìƒíƒœ | íŒŒì¼ ìœ„ì¹˜ |
|----------|------|----------|
| **CoNet** | âœ… êµ¬í˜„ ì™„ë£Œ | `baselines/conet/` |
| **DTCDR** | âœ… êµ¬í˜„ ì™„ë£Œ | `baselines/dtcdr/` |
| **LLM4CDR** | âœ… êµ¬í˜„ ì™„ë£Œ | `baselines/llm4cdr/` |
| **BaseEvaluator** | âœ… ê³µí†µ í´ë˜ìŠ¤ | `baselines/base_evaluator.py` |

### êµ¬í˜„ëœ ë©”íŠ¸ë¦­

| ë©”íŠ¸ë¦­ | ìƒíƒœ | íŒŒì¼ ìœ„ì¹˜ |
|--------|------|----------|
| Ranking Metrics | âœ… | `src/metrics/ranking_metrics.py` |
| Explainability Metrics | âœ… | `src/metrics/explainability_metrics.py` |
| Statistical Analysis | âœ… | `src/metrics/statistical_analysis.py` |
| GPT-4.1 Evaluation | âœ… | `src/metrics/explainability_metrics.py` | User Typeë³„ ê· ë“± ì¶”ì¶œ, ëª¨ë¸ë‹¹ 50ê°œ |

> âš ï¸ **RQ4 í‰ê°€ ëŒ€ìƒ**: KitREC ëª¨ë¸ë§Œ (Baseline ì œì™¸)
> - Baseline(CoNet, DTCDR, LLM4CDR)ì€ Confidence Score/MAE/RMSE ê³„ì‚° ë¶ˆí•„ìš”

---

## 1. í”„ë¡œì íŠ¸ í´ë” êµ¬ì¡°

```
Experimental_test/
â”‚
â”œâ”€â”€ CLAUDE.md                          # Claude Code ê°€ì´ë“œ ë¬¸ì„œ
â”œâ”€â”€ detail_task_plan.md                # ë³¸ ì‘ì—… ê³„íšì„œ
â”‚
â”œâ”€â”€ configs/                           # ì„¤ì • íŒŒì¼
â”‚   â”œâ”€â”€ eval_config.yaml               # í‰ê°€ ê³µí†µ ì„¤ì • (metrics, batch_size ë“±)
â”‚   â”œâ”€â”€ model_paths.yaml               # HuggingFace ëª¨ë¸ ê²½ë¡œ ë§¤í•‘
â”‚   â””â”€â”€ baseline_config.yaml           # Baseline ëª¨ë¸ë³„ ì„¤ì •
â”‚
â”œâ”€â”€ src/                               # ì†ŒìŠ¤ ì½”ë“œ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data/                          # ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ data_loader.py             # HuggingFace Hub ë°ì´í„° ë¡œë”©
â”‚   â”‚   â”œâ”€â”€ prompt_builder.py          # Inference í”„ë¡¬í”„íŠ¸ ìƒì„±
â”‚   â”‚   â””â”€â”€ candidate_handler.py       # Candidate Set ì²˜ë¦¬ (1 GT + 99 Neg)
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                        # ëª¨ë¸ ë¡œë”© ë° ì¶”ë¡ 
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ kitrec_model.py            # KitREC (Full/Direct) ëª¨ë¸ ë¡œë”
â”‚   â”‚   â”œâ”€â”€ base_model.py              # Base-CoT/Base-Direct ëª¨ë¸ ë¡œë”
â”‚   â”‚   â”œâ”€â”€ conet_wrapper.py           # CoNet ë² ì´ìŠ¤ë¼ì¸ ë˜í¼
â”‚   â”‚   â”œâ”€â”€ dtcdr_wrapper.py           # DTCDR ë² ì´ìŠ¤ë¼ì¸ ë˜í¼
â”‚   â”‚   â””â”€â”€ llm4cdr_wrapper.py         # LLM4CDR ë² ì´ìŠ¤ë¼ì¸ ë˜í¼
â”‚   â”‚
â”‚   â”œâ”€â”€ inference/                     # ì¶”ë¡  ì—”ì§„
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ vllm_inference.py          # vLLM ê¸°ë°˜ LLM ì¶”ë¡ 
â”‚   â”‚   â”œâ”€â”€ batch_inference.py         # ë°°ì¹˜ ì¶”ë¡  ê´€ë¦¬
â”‚   â”‚   â””â”€â”€ output_parser.py           # ëª¨ë¸ ì¶œë ¥ íŒŒì‹± (<think>, JSON)
â”‚   â”‚
â”‚   â”œâ”€â”€ metrics/                       # í‰ê°€ ì§€í‘œ ê³„ì‚°
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ ranking_metrics.py         # Hit@K, MRR, NDCG@K
â”‚   â”‚   â”œâ”€â”€ explainability_metrics.py  # MAE, RMSE, Perplexity
â”‚   â”‚   â””â”€â”€ stratified_analysis.py     # User Typeë³„, Metadataë³„ ë¶„ì„
â”‚   â”‚
â”‚   â””â”€â”€ utils/                         # ìœ í‹¸ë¦¬í‹°
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ logger.py                  # ë¡œê¹… ì„¤ì •
â”‚       â”œâ”€â”€ io_utils.py                # íŒŒì¼ I/O
â”‚       â””â”€â”€ visualization.py           # ê²°ê³¼ ì‹œê°í™”
â”‚
â”œâ”€â”€ scripts/                           # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ run_kitrec_eval.py             # KitREC ëª¨ë¸ í‰ê°€ ì‹¤í–‰
â”‚   â”œâ”€â”€ run_ablation_study.py          # RQ1: 2Ã—2 Ablation Study
â”‚   â”œâ”€â”€ run_baseline_eval.py           # Baseline ëª¨ë¸ í‰ê°€ ì‹¤í–‰
â”‚   â”œâ”€â”€ run_stratified_analysis.py     # User Typeë³„ ë¶„ì„
â”‚   â”œâ”€â”€ run_metadata_subgroup.py       # Movies Metadata ë¶„ë¦¬ í‰ê°€
â”‚   â””â”€â”€ generate_report.py             # ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±
â”‚
â”œâ”€â”€ baselines/                         # Baseline ëª¨ë¸ ì½”ë“œ
â”‚   â”œâ”€â”€ conet/                         # CoNet êµ¬í˜„ì²´
â”‚   â”‚   â”œâ”€â”€ model.py
â”‚   â”‚   â”œâ”€â”€ data_converter.py
â”‚   â”‚   â”œâ”€â”€ trainer.py
â”‚   â”‚   â””â”€â”€ evaluator.py
â”‚   â”œâ”€â”€ dtcdr/                         # DTCDR êµ¬í˜„ì²´
â”‚   â”‚   â”œâ”€â”€ model.py
â”‚   â”‚   â”œâ”€â”€ data_converter.py
â”‚   â”‚   â”œâ”€â”€ trainer.py
â”‚   â”‚   â””â”€â”€ evaluator.py
â”‚   â””â”€â”€ llm4cdr/                       # LLM4CDR êµ¬í˜„ì²´
â”‚       â”œâ”€â”€ prompts.py                 # 3-stage prompts
â”‚       â””â”€â”€ evaluator.py
â”‚
â”œâ”€â”€ results/                           # í‰ê°€ ê²°ê³¼ ì €ì¥
â”‚   â”œâ”€â”€ kitrec/                        # KitREC ê²°ê³¼
â”‚   â”‚   â”œâ”€â”€ dualft_movies_seta/
â”‚   â”‚   â”œâ”€â”€ dualft_movies_setb/
â”‚   â”‚   â”œâ”€â”€ dualft_music_seta/
â”‚   â”‚   â”œâ”€â”€ dualft_music_setb/
â”‚   â”‚   â”œâ”€â”€ singleft_movies_seta/
â”‚   â”‚   â”œâ”€â”€ singleft_movies_setb/
â”‚   â”‚   â”œâ”€â”€ singleft_music_seta/
â”‚   â”‚   â””â”€â”€ singleft_music_setb/
â”‚   â”œâ”€â”€ ablation/                      # RQ1 Ablation ê²°ê³¼
â”‚   â”‚   â”œâ”€â”€ kitrec_full/
â”‚   â”‚   â”œâ”€â”€ kitrec_direct/
â”‚   â”‚   â”œâ”€â”€ base_cot/
â”‚   â”‚   â””â”€â”€ base_direct/
â”‚   â”œâ”€â”€ baselines/                     # Baseline ê²°ê³¼
â”‚   â”‚   â”œâ”€â”€ conet/
â”‚   â”‚   â”œâ”€â”€ dtcdr/
â”‚   â”‚   â”œâ”€â”€ llm4cdr/
â”‚   â”‚   â””â”€â”€ vanilla_zeroshot/
â”‚   â””â”€â”€ reports/                       # ìµœì¢… ë¶„ì„ ë¦¬í¬íŠ¸
â”‚       â”œâ”€â”€ rq1_ablation_report.md
â”‚       â”œâ”€â”€ rq2_cdr_comparison.md
â”‚       â”œâ”€â”€ rq3_coldstart_analysis.md
â”‚       â”œâ”€â”€ rq4_explainability.md
â”‚       â””â”€â”€ final_paper_tables.md
â”‚
â”œâ”€â”€ logs/                              # ì‹¤í–‰ ë¡œê·¸
â”‚   â””â”€â”€ {timestamp}_eval.log
â”‚
â””â”€â”€ notebooks/                         # ë¶„ì„ ë…¸íŠ¸ë¶ (ì„ íƒ)
    â”œâ”€â”€ result_analysis.ipynb
    â””â”€â”€ visualization.ipynb
```

---

## 2. ì„¤ì • íŒŒì¼ ìƒì„¸ (`configs/`)

### 2.1 `eval_config.yaml`

```yaml
# í‰ê°€ ê³µí†µ ì„¤ì •
evaluation:
  batch_size: 8
  max_new_tokens: 2048
  temperature: 0.0  # Greedy decoding

metrics:
  ranking:
    - hit@1
    - hit@5
    - hit@10
    - mrr
    - ndcg@5
    - ndcg@10
  explainability:
    - mae
    - rmse
    - perplexity

# âš ï¸ Confidence Score ì •ê·œí™” (CLAUDE.md ì°¸ì¡°)
confidence_normalization:
  model_scale: 10  # ëª¨ë¸ ì¶œë ¥: 0~10
  gt_scale: 5      # Ground Truth: 0~5
  divisor: 2       # confidence / 2

# í›„ë³´êµ° ì™¸ item_id ì²˜ë¦¬
invalid_item_handling:
  action: "fail"   # rank = âˆ
  log_errors: true
```

### 2.2 `model_paths.yaml`

```yaml
# HuggingFace ëª¨ë¸ ê²½ë¡œ ë§¤í•‘
base_model:
  name: "Qwen/Qwen3-14B"

kitrec_models:
  dualft_movies_seta: "Younggooo/kitrec-dualft-movies-seta-model"
  dualft_movies_setb: "Younggooo/kitrec-dualft-movies-setb-model"
  dualft_music_seta: "Younggooo/kitrec-dualft-music-seta-model"
  dualft_music_setb: "Younggooo/kitrec-dualft-music-setb-model"
  singleft_movies_seta: "Younggooo/kitrec-singleft-movies-seta-model"
  singleft_movies_setb: "Younggooo/kitrec-singleft-movies-setb-model"
  singleft_music_seta: "Younggooo/kitrec-singleft-music-seta-model"
  singleft_music_setb: "Younggooo/kitrec-singleft-music-setb-model"

datasets:
  test_seta: "Younggooo/kitrec-test-seta"
  test_setb: "Younggooo/kitrec-test-setb"
```

### 2.3 `baseline_config.yaml`

```yaml
# Baseline ëª¨ë¸ ì„¤ì •
conet:
  hidden_dim: 256
  num_layers: 3
  learning_rate: 0.001

dtcdr:
  embedding_dim: 128
  mlp_layers: [256, 128]

llm4cdr:
  model: "Qwen/Qwen3-14B"
  stages:
    - domain_gap_analysis
    - user_interest_reasoning
    - candidate_reranking
```

---

## 3. ì†ŒìŠ¤ ì½”ë“œ ìƒì„¸ ëª…ì„¸ (`src/`)

### 3.1 `src/data/data_loader.py`

```python
"""
HuggingFace Hubì—ì„œ Test Set ë¡œë”©
âš ï¸ CLAUDE.md Critical Notes #1: Template Schema Difference ì ìš©
"""

from datasets import load_dataset

class DataLoader:
    def __init__(self, dataset_name: str, hf_token: str = None):
        self.dataset_name = dataset_name
        self.hf_token = hf_token

    def load_test_data(self):
        """Test ë°ì´í„° ë¡œë”© ë° í”„ë¡¬í”„íŠ¸ ì¶”ì¶œ"""
        dataset = load_dataset(self.dataset_name, token=self.hf_token)
        return dataset["train"]  # test split

    def extract_prompt(self, sample: dict) -> str:
        """
        âš ï¸ Template Schema Difference (CLAUDE.md í•„ìˆ˜ íŒ¨í„´):
        - Val/Test: `input` í•„ë“œì— ì „ì²´ í”„ë¡¬í”„íŠ¸
        - Training: `instruction` í•„ë“œì— ì „ì²´ í”„ë¡¬í”„íŠ¸
        """
        prompt = sample["input"] if sample.get("input") else sample["instruction"]
        return prompt

    def extract_ground_truth(self, sample: dict) -> dict:
        """Ground Truth ì•„ì´í…œ ì •ë³´ ì¶”ì¶œ"""
        return sample["ground_truth"]

    def extract_candidate_ids(self, sample: dict) -> list:
        """Candidate Setì˜ item_id ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ (ê²€ì¦ìš©)"""
        # í”„ë¡¬í”„íŠ¸ì—ì„œ (ID: xxx) íŒ¨í„´ ì¶”ì¶œ
        import re
        prompt = self.extract_prompt(sample)
        pattern = r'\(ID:\s*([A-Z0-9]+)\)'
        return re.findall(pattern, prompt)
```

### 3.2 `src/data/prompt_builder.py`

```python
"""
Inference í”„ë¡¬í”„íŠ¸ ìƒì„±
âš ï¸ CLAUDE.md RQ1 Note: Direct ëª¨ë¸ìš© Reasoning Guidelines ì œê±° ë²„ì „ í•„ìš”
"""

class PromptBuilder:
    # CLAUDE.mdì˜ Inference Prompt Template ì „ì²´
    THINKING_TEMPLATE = '''# Expert Cross-Domain Recommendation System

You are a specialized recommendation system with expertise in cross-domain knowledge transfer.
Your task is to leverage comprehensive user interaction patterns from source and target domains to rank the **Top 10** most suitable items from the candidate list.

## Input Parameters
- Source Domain: {source_domain}
- Target Domain: {target_domain}
- Task: Rank the top 10 items based on user preference alignment.

## User Interaction History
{user_history}

## List of Available Candidate Items (Total 100):
{candidate_list}

## Reasoning Guidelines (Thinking Process)
Before generating the final JSON output, you must engage in a deep reasoning process.
Think step-by-step using the following phases:

### Phase 1: Pattern Recognition (Source Domain Analysis)
- Analyze the user's `{source_domain}` history to identify core preference signals.
- Extract key genres, thematic interests, content complexity, and stylistic preferences.
- Identify high-rated items (Rating > 4.0) to understand what the user truly values.

### Phase 2: Cross-Domain Knowledge Transfer
- Apply domain knowledge to map preferences from `{source_domain}` to `{target_domain}`.
- Consider semantic connections, author/director styles, and emotional tone.

### Phase 3: Candidate Evaluation & Selection
- Evaluate the 100 candidate items against the transferred profile.
- Select the Top 10 items that best match the inferred preferences.
- Ensure diversity in the selection while maintaining high relevance.
- Formulate a rationale for each selected item.

## Output Format
After your reasoning process, provide results **ONLY** as a JSON array containing the **Top-10** recommended items.

```json
[
   {{ "rank": 1, "item_id": "...", "title": "...", "confidence_score": <float 1-10>, "rationale": "..." }},
   ...
   {{ "rank": 10, "item_id": "...", "title": "...", "confidence_score": <float 1-10>, "rationale": "..." }}
]
```
'''

    # Direct ëª¨ë¸ìš© (Reasoning Guidelines ì œê±°)
    DIRECT_TEMPLATE = '''# Expert Cross-Domain Recommendation System

You are a specialized recommendation system with expertise in cross-domain knowledge transfer.
Your task is to leverage comprehensive user interaction patterns from source and target domains to rank the **Top 10** most suitable items from the candidate list.

## Input Parameters
- Source Domain: {source_domain}
- Target Domain: {target_domain}
- Task: Rank the top 10 items based on user preference alignment.

## User Interaction History
{user_history}

## List of Available Candidate Items (Total 100):
{candidate_list}

## Output Format
Provide results **ONLY** as a JSON array containing the **Top-10** recommended items.
Do NOT include any reasoning or thinking process.

```json
[
   {{ "rank": 1, "item_id": "...", "title": "...", "confidence_score": <float 1-10>, "rationale": "..." }},
   ...
   {{ "rank": 10, "item_id": "...", "title": "...", "confidence_score": <float 1-10>, "rationale": "..." }}
]
```
'''

    def build_thinking_prompt(self, sample: dict) -> str:
        """
        KitREC-Full, Base-CoTìš© (Reasoning Guidelines í¬í•¨)
        - í•™ìŠµëœ ëª¨ë¸ ë˜ëŠ” Zero-shot CoT í‰ê°€ì— ì‚¬ìš©
        """
        # Val/Test ë°ì´í„°ëŠ” ì´ë¯¸ ì™„ì„±ëœ í”„ë¡¬í”„íŠ¸ê°€ inputì— ìˆìŒ
        return sample["input"] if sample.get("input") else sample["instruction"]

    def build_direct_prompt(self, sample: dict) -> str:
        """
        KitREC-Direct, Base-Directìš© (Reasoning Guidelines ì œê±°)
        - Thinking Process ì—†ì´ ë°”ë¡œ JSON ì¶œë ¥
        """
        original_prompt = sample["input"] if sample.get("input") else sample["instruction"]

        # "## Reasoning Guidelines" ì„¹ì…˜ ì œê±°
        import re
        pattern = r'## Reasoning Guidelines.*?(?=## Output Format)'
        modified = re.sub(pattern, '', original_prompt, flags=re.DOTALL)

        # Output Format ìˆ˜ì • (reasoning ì—†ì´ ë°”ë¡œ ì¶œë ¥)
        modified = modified.replace(
            "After your reasoning process, provide results",
            "Provide results directly"
        )

        return modified
```

### 3.3 `src/data/candidate_handler.py`

```python
"""
Candidate Set ì²˜ë¦¬ ë° ê²€ì¦
âš ï¸ CLAUDE.md Baseline ê³µì •ì„± ì¡°ê±´: ëª¨ë“  ëª¨ë¸ ë™ì¼ Candidate Set ì‚¬ìš©
"""

import re
from typing import List, Set

class CandidateHandler:
    def extract_candidate_ids(self, prompt: str) -> List[str]:
        """í”„ë¡¬í”„íŠ¸ì—ì„œ Candidate item_id ì¶”ì¶œ"""
        pattern = r'\(ID:\s*([A-Z0-9]+)\)'
        return re.findall(pattern, prompt)

    def validate_prediction(self, predicted_id: str, candidate_ids: List[str]) -> bool:
        """
        ì˜ˆì¸¡ëœ item_idê°€ Candidate Setì— ìˆëŠ”ì§€ ê²€ì¦
        âš ï¸ í›„ë³´êµ° ì™¸ item ì¶œë ¥ ì‹œ â†’ ìë™ fail ì²˜ë¦¬ (rank = âˆ)
        """
        return predicted_id in candidate_ids

    def convert_to_id_matrix(self, user_history: List[str], item_vocab: dict) -> List[int]:
        """
        Baseline ëª¨ë¸ìš©: í…ìŠ¤íŠ¸ History â†’ ID matrix ë³€í™˜
        âš ï¸ CLAUDE.md Critical Notes #4: ë™ì¼ ì‹œì  ë°ì´í„° ì‚¬ìš© í•„ìˆ˜
        """
        return [item_vocab.get(item_id, 0) for item_id in user_history]
```

### 3.4 `src/inference/output_parser.py`

```python
"""
ëª¨ë¸ ì¶œë ¥ íŒŒì‹±
âš ï¸ CLAUDE.md Critical Notes #3: Output Parsing ì£¼ì˜ì‚¬í•­ ì ìš©
"""

import re
import json
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass

@dataclass
class ParseResult:
    thinking: Optional[str]
    predictions: List[Dict]
    parse_errors: List[str]
    invalid_items: List[str]  # í›„ë³´êµ° ì™¸ item_id

class OutputParser:
    def parse(self, raw_output: str, candidate_ids: List[str]) -> ParseResult:
        """
        ëª¨ë¸ ì¶œë ¥ íŒŒì‹±

        1. <think>...</think> ë¸”ë¡ ë¶„ë¦¬
        2. JSON ë¸”ë¡ ì¶”ì¶œ (```json ... ```)
        3. trailing comma ì œê±°
        4. item_id ê²€ì¦: candidate_idsì— ì—†ìœ¼ë©´ fail (rank=âˆ)
        5. ì˜¤ë¥˜ìœ¨ í†µê³„ ë°˜í™˜
        """
        errors = []
        invalid_items = []

        # 1. Thinking ë¸”ë¡ ë¶„ë¦¬
        thinking = self._extract_thinking(raw_output)

        # 2. JSON ë¸”ë¡ ì¶”ì¶œ
        json_str = self._extract_json(raw_output)
        if not json_str:
            errors.append("JSON block not found")
            return ParseResult(thinking, [], errors, [])

        # 3. Trailing comma ì œê±°
        json_str = self._remove_trailing_comma(json_str)

        # 4. JSON íŒŒì‹±
        try:
            predictions = json.loads(json_str)
        except json.JSONDecodeError as e:
            errors.append(f"JSON parse error: {str(e)}")
            return ParseResult(thinking, [], errors, [])

        # 5. item_id ê²€ì¦
        valid_predictions = []
        for pred in predictions:
            item_id = pred.get("item_id", "")
            if item_id in candidate_ids:
                valid_predictions.append(pred)
            else:
                invalid_items.append(item_id)
                errors.append(f"Invalid item_id: {item_id} (not in candidate set)")

        return ParseResult(thinking, valid_predictions, errors, invalid_items)

    def _extract_thinking(self, output: str) -> Optional[str]:
        """<think>...</think> ë¸”ë¡ ì¶”ì¶œ"""
        pattern = r'<think>(.*?)</think>'
        match = re.search(pattern, output, re.DOTALL)
        return match.group(1).strip() if match else None

    def _extract_json(self, output: str) -> Optional[str]:
        """
        ```json ... ``` ë¸”ë¡ ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)
        - ë‹¤ì¤‘ ë¼ì¸ JSON ì²˜ë¦¬ ìœ„í•´ [\s\S] ì‚¬ìš©
        - ë°°ì—´ ë° ê°ì²´ í˜•ì‹ ëª¨ë‘ ì§€ì›
        """
        # Priority 1: ```json ì½”ë“œ ë¸”ë¡ (ë°°ì—´)
        pattern = r'```json\s*([\[\{][\s\S]*?[\]\}])\s*```'
        match = re.search(pattern, output)
        if match:
            return match.group(1).strip()

        # Priority 2: ``` ì½”ë“œ ë¸”ë¡ (json íƒœê·¸ ì—†ìŒ)
        pattern = r'```\s*([\[\{][\s\S]*?[\]\}])\s*```'
        match = re.search(pattern, output)
        if match:
            return match.group(1).strip()

        # Priority 3: ì½”ë“œ ë¸”ë¡ ì—†ì´ JSON ë°°ì—´ ì§ì ‘ ì°¾ê¸°
        pattern = r'\[[\s\S]*?\{[\s\S]*?\}[\s\S]*?\]'
        match = re.search(pattern, output)
        if match:
            return match.group(0)

        # Priority 4: ë‹¨ì¼ JSON ê°ì²´
        pattern = r'\{[\s\S]*?\}'
        match = re.search(pattern, output)
        return match.group(0) if match else None

    def _remove_trailing_comma(self, json_str: str) -> str:
        """trailing comma ì œê±°"""
        # },] ë˜ëŠ” }, ] íŒ¨í„´ ì²˜ë¦¬
        json_str = re.sub(r',\s*\]', ']', json_str)
        json_str = re.sub(r',\s*\}', '}', json_str)
        return json_str


class ErrorStatistics:
    """íŒŒì‹± ì˜¤ë¥˜ í†µê³„"""
    def __init__(self):
        self.total_samples = 0
        self.parse_failures = 0
        self.invalid_item_count = 0
        self.invalid_item_ids = []

    def update(self, result: ParseResult):
        self.total_samples += 1
        if result.parse_errors:
            self.parse_failures += 1
        self.invalid_item_count += len(result.invalid_items)
        self.invalid_item_ids.extend(result.invalid_items)

    def get_summary(self) -> dict:
        return {
            "total_samples": self.total_samples,
            "parse_failure_rate": self.parse_failures / max(self.total_samples, 1),
            "invalid_item_rate": self.invalid_item_count / max(self.total_samples, 1),
            "unique_invalid_items": len(set(self.invalid_item_ids))
        }
```

### 3.5 `src/metrics/ranking_metrics.py`

```python
"""
ë­í‚¹ í‰ê°€ ì§€í‘œ
âš ï¸ CLAUDE.md Evaluation Metrics ì°¸ì¡°
"""

import numpy as np
from typing import List, Dict

class RankingMetrics:
    @staticmethod
    def hit_at_k(predictions: List[Dict], ground_truth_id: str, k: int) -> float:
        """
        Hit@K: Top-K ì•ˆì— ì •ë‹µ í¬í•¨ ì—¬ë¶€
        - Hit@1: ì •í™•íˆ 1ìœ„ë¡œ ì˜ˆì¸¡
        - Hit@5: Top-5 ì•ˆì— ì •ë‹µ í¬í•¨
        - Hit@10: Top-10 ì•ˆì— ì •ë‹µ í¬í•¨
        """
        top_k_ids = [p["item_id"] for p in predictions[:k]]
        return 1.0 if ground_truth_id in top_k_ids else 0.0

    @staticmethod
    def mrr(predictions: List[Dict], ground_truth_id: str) -> float:
        """
        Mean Reciprocal Rank
        - 1ìœ„=1.0, 2ìœ„=0.5, 3ìœ„=0.33, 10ìœ„=0.1
        - ì •ë‹µì´ ì—†ìœ¼ë©´ 0
        """
        for i, pred in enumerate(predictions):
            if pred["item_id"] == ground_truth_id:
                return 1.0 / (i + 1)
        return 0.0

    @staticmethod
    def ndcg_at_k(predictions: List[Dict], ground_truth_id: str, k: int) -> float:
        """
        NDCG@K: Normalized Discounted Cumulative Gain
        - 1ìœ„=1.0, 2ìœ„=0.631, 10ìœ„=0.289 (log2 ê¸°ë°˜)
        """
        dcg = 0.0
        for i, pred in enumerate(predictions[:k]):
            if pred["item_id"] == ground_truth_id:
                # relevance = 1 for ground truth
                dcg = 1.0 / np.log2(i + 2)  # i+2 because log2(1) = 0
                break

        # IDCG: ì´ìƒì ì¸ ê²½ìš° (ì •ë‹µì´ 1ìœ„)
        idcg = 1.0 / np.log2(2)  # = 1.0

        return dcg / idcg if idcg > 0 else 0.0

    @staticmethod
    def calculate_all(predictions: List[Dict], ground_truth_id: str) -> Dict[str, float]:
        """ëª¨ë“  ë­í‚¹ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        return {
            "hit@1": RankingMetrics.hit_at_k(predictions, ground_truth_id, 1),
            "hit@5": RankingMetrics.hit_at_k(predictions, ground_truth_id, 5),
            "hit@10": RankingMetrics.hit_at_k(predictions, ground_truth_id, 10),
            "mrr": RankingMetrics.mrr(predictions, ground_truth_id),
            "ndcg@5": RankingMetrics.ndcg_at_k(predictions, ground_truth_id, 5),
            "ndcg@10": RankingMetrics.ndcg_at_k(predictions, ground_truth_id, 10),
        }
```

### 3.6 `src/metrics/explainability_metrics.py`

```python
"""
ì„¤ëª…ë ¥ í‰ê°€ ì§€í‘œ
âš ï¸ CLAUDE.md Evaluation Metrics: Confidence Score ì •ê·œí™” í•„ìˆ˜
"""

import numpy as np
from typing import List, Dict
import torch

class ExplainabilityMetrics:
    def __init__(self, confidence_divisor: float = 2.0):
        """
        âš ï¸ Confidence Score ì •ê·œí™”:
        - Model ì¶œë ¥: 0~10 float
        - Ground Truth: 0~5 rating
        - ì •ê·œí™”: confidence / 2
        """
        self.confidence_divisor = confidence_divisor

    def normalize_confidence(self, confidence: float) -> float:
        """Confidence Score ì •ê·œí™”"""
        return confidence / self.confidence_divisor

    def mae(self, predictions: List[Dict], ground_truth_ratings: List[float]) -> float:
        """
        Mean Absolute Error
        ì˜ˆì¸¡ ì‹ ë¢°ë„ì™€ ì‹¤ì œ Rating ë¹„êµ
        """
        errors = []
        for pred, gt_rating in zip(predictions, ground_truth_ratings):
            normalized_conf = self.normalize_confidence(pred.get("confidence_score", 5.0))
            errors.append(abs(normalized_conf - gt_rating))
        return np.mean(errors) if errors else 0.0

    def rmse(self, predictions: List[Dict], ground_truth_ratings: List[float]) -> float:
        """
        Root Mean Squared Error
        """
        squared_errors = []
        for pred, gt_rating in zip(predictions, ground_truth_ratings):
            normalized_conf = self.normalize_confidence(pred.get("confidence_score", 5.0))
            squared_errors.append((normalized_conf - gt_rating) ** 2)
        return np.sqrt(np.mean(squared_errors)) if squared_errors else 0.0

    def perplexity(self, model, tokenizer, rationales: List[str]) -> float:
        """
        Perplexity: ì¶”ì²œ ì„¤ëª…ì˜ ì–¸ì–´ì  í’ˆì§ˆ í‰ê°€
        ë‚®ì„ìˆ˜ë¡ ëª¨ë¸ì´ í™•ì‹ ì„ ê°€ì§€ê³  ìƒì„±
        """
        total_loss = 0.0
        total_tokens = 0

        for rationale in rationales:
            inputs = tokenizer(rationale, return_tensors="pt")
            with torch.no_grad():
                outputs = model(**inputs, labels=inputs["input_ids"])
                total_loss += outputs.loss.item() * inputs["input_ids"].size(1)
                total_tokens += inputs["input_ids"].size(1)

        avg_loss = total_loss / total_tokens if total_tokens > 0 else 0
        return np.exp(avg_loss)
```

### 3.7 `src/metrics/stratified_analysis.py`

```python
"""
Stratified ë¶„ì„
âš ï¸ CLAUDE.md: User Typeë³„, Metadataë³„ ë¶„ë¦¬ ë¶„ì„
"""

from typing import List, Dict
from collections import defaultdict

class StratifiedAnalysis:
    # User Type ì •ì˜ (CLAUDE.md ì°¸ì¡°)
    USER_TYPE_MAPPING = {
        "source_only_movies": {"core": 1, "model": "SingleFT-Movies"},
        "source_only_music": {"core": 1, "model": "SingleFT-Music"},
        "cold_start_2core_movies": {"core": 2, "model": "DualFT-Movies"},
        "cold_start_2core_music": {"core": 2, "model": "DualFT-Music"},
        "cold_start_3core_movies": {"core": 3, "model": "DualFT-Movies"},
        "cold_start_3core_music": {"core": 3, "model": "DualFT-Music"},
        "cold_start_4core_movies": {"core": 4, "model": "DualFT-Movies"},
        "cold_start_4core_music": {"core": 4, "model": "DualFT-Music"},
        "overlapping_books_movies": {"core": "5+", "model": "DualFT-Movies"},
        "overlapping_books_music": {"core": "5+", "model": "DualFT-Music"},
    }

    def analyze_by_user_type(self, results: List[Dict]) -> Dict[str, Dict]:
        """User Typeë³„ ì„±ëŠ¥ ë¶„ì„"""
        grouped = defaultdict(list)

        for result in results:
            user_type = result["metadata"]["user_type"]
            grouped[user_type].append(result["metrics"])

        analysis = {}
        for user_type, metrics_list in grouped.items():
            analysis[user_type] = self._aggregate_metrics(metrics_list)
            analysis[user_type]["core_level"] = self.USER_TYPE_MAPPING.get(
                user_type, {}
            ).get("core", "unknown")

        return analysis

    def analyze_by_core_level(self, results: List[Dict]) -> Dict[str, Dict]:
        """Core Levelë³„ ì„±ëŠ¥ ë¶„ì„ (1-core ~ 10-core)"""
        grouped = defaultdict(list)

        for result in results:
            user_type = result["metadata"]["user_type"]
            core = self.USER_TYPE_MAPPING.get(user_type, {}).get("core", "unknown")
            grouped[f"{core}-core"].append(result["metrics"])

        return {
            level: self._aggregate_metrics(metrics_list)
            for level, metrics_list in grouped.items()
        }

    def analyze_by_metadata_availability(
        self,
        results: List[Dict],
        metadata_lookup: Dict[str, bool]
    ) -> Dict[str, Dict]:
        """
        Movies Metadata ë¶„ë¦¬ í‰ê°€ (CLAUDE.md Sub-group Analysis)
        - Group A: Target Items with Metadata (Title/Category ì¡´ì¬)
        - Group B: Target Items without Metadata (Unknown)
        """
        group_a = []  # Metadata ìˆìŒ
        group_b = []  # Metadata ì—†ìŒ (Unknown)

        for result in results:
            gt_item_id = result["ground_truth"]["item_id"]
            has_metadata = metadata_lookup.get(gt_item_id, False)

            if has_metadata:
                group_a.append(result["metrics"])
            else:
                group_b.append(result["metrics"])

        return {
            "group_a_with_metadata": {
                **self._aggregate_metrics(group_a),
                "count": len(group_a)
            },
            "group_b_unknown": {
                **self._aggregate_metrics(group_b),
                "count": len(group_b)
            }
        }

    def _aggregate_metrics(self, metrics_list: List[Dict]) -> Dict[str, float]:
        """ë©”íŠ¸ë¦­ ì§‘ê³„ (í‰ê· )"""
        if not metrics_list:
            return {}

        aggregated = defaultdict(list)
        for metrics in metrics_list:
            for key, value in metrics.items():
                aggregated[key].append(value)

        return {
            key: sum(values) / len(values)
            for key, values in aggregated.items()
        }
```

### 3.8 `src/metrics/statistical_analysis.py`

```python
"""
í†µê³„ì  ìœ ì˜ì„± ê²€ì •
âš ï¸ CLAUDE.md: ë…¼ë¬¸ ë°œí‘œë¥¼ ìœ„í•´ ëª¨ë“  Baseline ë¹„êµì—ì„œ í†µê³„ì  ìœ ì˜ì„± ë³´ê³  í•„ìˆ˜
"""

from scipy import stats
import numpy as np
from typing import List, Dict

class StatisticalAnalysis:
    @staticmethod
    def paired_t_test(scores_a: List[float], scores_b: List[float]) -> Dict:
        """
        Paired t-test for per-sample metric comparison
        - RQ1: KitREC-Full vs Ablation models
        - RQ2: KitREC vs Baselines
        """
        t_stat, p_value = stats.ttest_rel(scores_a, scores_b)

        # Cohen's d effect size
        diff = np.array(scores_a) - np.array(scores_b)
        effect_size = np.mean(diff) / np.std(diff) if np.std(diff) > 0 else 0

        return {
            "t_statistic": float(t_stat),
            "p_value": float(p_value),
            "significant_at_0.05": p_value < 0.05,
            "significant_at_0.01": p_value < 0.01,
            "significant_at_0.001": p_value < 0.001,
            "effect_size_cohens_d": float(effect_size),
            "mean_diff": float(np.mean(diff)),
            "n_samples": len(scores_a)
        }

    @staticmethod
    def bootstrap_ci(scores: List[float], n_bootstrap: int = 1000, ci: float = 0.95) -> Dict:
        """Bootstrap confidence interval for single metric"""
        bootstrapped = [
            np.mean(np.random.choice(scores, size=len(scores), replace=True))
            for _ in range(n_bootstrap)
        ]
        alpha = (1 - ci) / 2
        return {
            "mean": float(np.mean(scores)),
            "std": float(np.std(scores)),
            "ci_lower": float(np.percentile(bootstrapped, alpha * 100)),
            "ci_upper": float(np.percentile(bootstrapped, (1 - alpha) * 100)),
            "ci_level": ci
        }

    @staticmethod
    def compare_all_baselines(kitrec_scores: Dict[str, List[float]],
                               baseline_scores: Dict[str, Dict[str, List[float]]]) -> Dict:
        """
        Compare KitREC against all baselines for all metrics
        Returns: {baseline_name: {metric: t_test_result}}
        """
        results = {}
        for baseline_name, baseline_metrics in baseline_scores.items():
            results[baseline_name] = {}
            for metric, kitrec_metric_scores in kitrec_scores.items():
                if metric in baseline_metrics:
                    results[baseline_name][metric] = StatisticalAnalysis.paired_t_test(
                        kitrec_metric_scores,
                        baseline_metrics[metric]
                    )
        return results

    @staticmethod
    def format_for_paper(result: Dict) -> str:
        """Format t-test result for paper table (e.g., 0.85* or 0.85**)"""
        mean_diff = result["mean_diff"]
        if result["significant_at_0.001"]:
            return f"{mean_diff:+.3f}***"
        elif result["significant_at_0.01"]:
            return f"{mean_diff:+.3f}**"
        elif result["significant_at_0.05"]:
            return f"{mean_diff:+.3f}*"
        else:
            return f"{mean_diff:+.3f}"
```

---

## 4. Baseline ëª¨ë¸ ìƒì„¸ (`baselines/`)

### ğŸ”’ 4.0 Baseline Train/Test ë°ì´í„° ë¶„ë¦¬ (Data Leakage ë°©ì§€)

> **âš ï¸ í•µì‹¬ ì›ì¹™:** Baseline ëª¨ë¸ì€ ë°˜ë“œì‹œ KitREC Training ë°ì´í„°ë¡œ í•™ìŠµí•˜ê³ , KitREC Test ë°ì´í„°ë¡œ í‰ê°€í•´ì•¼ í•©ë‹ˆë‹¤.
> ê¸°ì¡´ HuggingFace ë°ì´í„°ì…‹ì€ ì´ë¯¸ Train/Testê°€ ì—„ê²©íˆ ë¶„ë¦¬ë˜ì–´ ìˆìœ¼ë¯€ë¡œ Data Leakage ë¬¸ì œê°€ ì—†ìŠµë‹ˆë‹¤.

#### Baseline í•™ìŠµìš© ë°ì´í„°ì…‹ ë§¤í•‘

| Target Domain | Candidate Set | Training Dataset | Samples |
|---------------|---------------|------------------|---------|
| Movies | Set A | `Younggooo/kitrec-dualft_movies-seta` | 12,000 |
| Movies | Set B | `Younggooo/kitrec-dualft_movies-setb` | 12,000 |
| Music | Set A | `Younggooo/kitrec-dualft_music-seta` | 12,000 |
| Music | Set B | `Younggooo/kitrec-dualft_music-setb` | 12,000 |

#### Baseline í‰ê°€ìš© ë°ì´í„°ì…‹

| Candidate Set | Test Dataset | Samples | ë‚´ìš© |
|---------------|--------------|---------|------|
| Set A | `Younggooo/kitrec-test-seta` | 30,000 | Hard Negatives |
| Set B | `Younggooo/kitrec-test-setb` | 30,000 | Random Negatives |

#### ì˜¬ë°”ë¥¸ Baseline ì‹¤í—˜ ì½”ë“œ

```python
# âœ… ì˜¬ë°”ë¥¸ ë°©ë²•: Train/Test ì™„ì „ ë¶„ë¦¬
def run_baseline_experiment(args):
    # Step 1: í•™ìŠµ ë°ì´í„° ë¡œë“œ (KitREC Training Dataset)
    train_dataset = f"Younggooo/kitrec-dualft_{args.target_domain}-{args.candidate_set}"
    train_loader = DataLoader(train_dataset, hf_token=args.hf_token)
    train_data = train_loader.load_test_data()

    # Step 2: Baseline ëª¨ë¸ í•™ìŠµ
    if args.train_baseline:
        trainer.train(train_data)
        trainer.save_checkpoint(f"checkpoints/{args.baseline}_{args.target_domain}_{args.candidate_set}.pt")

    # Step 3: í‰ê°€ ë°ì´í„° ë¡œë“œ (KitREC Test Dataset) - ë³„ë„ ë‹¨ê³„
    test_dataset = f"Younggooo/kitrec-test-{args.candidate_set}"
    test_loader = DataLoader(test_dataset, hf_token=args.hf_token)
    test_data = test_loader.load_test_data()

    # Step 4: Baseline í‰ê°€ (Test ë°ì´í„°ë§Œ ì‚¬ìš©)
    evaluator.evaluate(test_data)
```

```python
# âŒ ê¸ˆì§€: Test ë°ì´í„°ë¥¼ ë¶„í• í•˜ì—¬ í•™ìŠµì— ì‚¬ìš© (Data Leakage)
# ì•„ë˜ì™€ ê°™ì€ ì½”ë“œëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤!
train_samples = converted_samples[:-1000]  # ì˜ëª»ë¨!
val_samples = converted_samples[-1000:]    # ì˜ëª»ë¨!
```

#### Baseline ì‹¤í—˜ ì‹¤í–‰ ì˜ˆì‹œ

```bash
# CoNet í•™ìŠµ (Train ë°ì´í„°)
python scripts/run_baseline_train.py \
    --baseline conet \
    --target_domain movies \
    --candidate_set seta \
    --train_dataset Younggooo/kitrec-dualft_movies-seta

# CoNet í‰ê°€ (Test ë°ì´í„°)
python scripts/run_baseline_eval.py \
    --baseline conet \
    --target_domain movies \
    --candidate_set seta \
    --baseline_checkpoint checkpoints/conet_movies_seta.pt
```

### 4.1 `baselines/conet/data_converter.py`

```python
"""
CoNetìš© ë°ì´í„° ë³€í™˜
âš ï¸ CLAUDE.md Critical Notes #4: ë™ì¼ User History ì‹œí€€ìŠ¤ ì‚¬ìš© í•„ìˆ˜
"""

class CoNetDataConverter:
    def __init__(self, item_vocab: Dict[str, int]):
        self.item_vocab = item_vocab

    def convert_history(self, text_history: List[str]) -> np.ndarray:
        """
        í…ìŠ¤íŠ¸ History â†’ ID matrix ë³€í™˜
        âš ï¸ KitRECì— ë“¤ì–´ê°€ëŠ” Historyì™€ ë™ì¼í•œ ì‹œì ì˜ ë°ì´í„° ì‚¬ìš©
        """
        ids = [self.item_vocab.get(item_id, 0) for item_id in text_history]
        return np.array(ids)

    def convert_candidates(self, candidate_ids: List[str]) -> np.ndarray:
        """
        Candidate Set ë³€í™˜
        âš ï¸ ë°˜ë“œì‹œ KitRECê³¼ ë™ì¼í•œ 100ê°œ í›„ë³´ (1 GT + 99 Neg) ì‚¬ìš©
        """
        return np.array([self.item_vocab.get(cid, 0) for cid in candidate_ids])
```

### 4.2 `baselines/llm4cdr/prompts.py`

```python
"""
LLM4CDR 3-stage íŒŒì´í”„ë¼ì¸ í”„ë¡¬í”„íŠ¸
"""

class LLM4CDRPrompts:
    STAGE1_DOMAIN_GAP = """
Analyze the relationship between {source_domain} and {target_domain} domains.
What semantic connections exist between these two domains?
"""

    STAGE2_USER_INTEREST = """
Based on the user's {source_domain} history:
{user_history}

Describe the user's preferences and interests that might transfer to {target_domain}.
"""

    STAGE3_RERANKING = """
Given the user's inferred preferences and the following candidate items:
{candidates}

Re-rank these items based on the user's likely preferences.
Return the top 10 items in JSON format.
"""
```

---

## 5. ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ìƒì„¸ (`scripts/`)

### 5.1 `scripts/run_kitrec_eval.py`

```python
"""
KitREC ëª¨ë¸ í‰ê°€ ì‹¤í–‰
8ê°œ ëª¨ë¸ Ã— 30,000 ìƒ˜í”Œ í‰ê°€
"""

import argparse
from src.data.data_loader import DataLoader
from src.models.kitrec_model import KitRECModel
from src.inference.vllm_inference import VLLMInference
from src.inference.output_parser import OutputParser, ErrorStatistics
from src.metrics.ranking_metrics import RankingMetrics
from src.metrics.explainability_metrics import ExplainabilityMetrics

def main(args):
    # 1. ë°ì´í„° ë¡œë”©
    loader = DataLoader(f"Younggooo/kitrec-test-{args.set}")
    test_data = loader.load_test_data()

    # 2. ëª¨ë¸ ë¡œë”©
    model = KitRECModel.load(args.model_name)

    # 3. ì¶”ë¡  ì—”ì§„ ì´ˆê¸°í™”
    inference = VLLMInference(model)
    parser = OutputParser()
    error_stats = ErrorStatistics()

    # 4. í‰ê°€ ì‹¤í–‰
    results = []
    for sample in tqdm(test_data):
        prompt = loader.extract_prompt(sample)
        candidate_ids = loader.extract_candidate_ids(sample)
        gt = loader.extract_ground_truth(sample)

        # ì¶”ë¡ 
        output = inference.generate(prompt)

        # íŒŒì‹±
        parse_result = parser.parse(output, candidate_ids)
        error_stats.update(parse_result)

        # ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = RankingMetrics.calculate_all(
            parse_result.predictions,
            gt["item_id"]
        )

        results.append({
            "sample_id": sample["user_id"],
            "predictions": parse_result.predictions,
            "ground_truth": gt,
            "metrics": metrics,
            "metadata": sample["metadata"]
        })

    # 5. ê²°ê³¼ ì €ì¥
    save_results(results, args.output_dir)
    print(f"Error Statistics: {error_stats.get_summary()}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_name", required=True)
    parser.add_argument("--set", choices=["seta", "setb"], required=True)
    parser.add_argument("--output_dir", default="results/kitrec")
    args = parser.parse_args()
    main(args)
```

### 5.2 `scripts/run_ablation_study.py`

```python
"""
RQ1: 2Ã—2 Ablation Study
âš ï¸ CLAUDE.md RQ1 ìƒì„¸ ì •ì˜ ì°¸ì¡°
âš ï¸ KitREC-DirectëŠ” Option A (ë³„ë„ í•™ìŠµ) ê¶Œì¥
"""

import re
from src.data.prompt_builder import PromptBuilder

def run_ablation():
    """
    4ê°œ ëª¨ë¸ ë¹„êµ:
    â‘  KitREC-Full (ì œì•ˆ ëª¨ë¸) - Thinking + Fine-tuned
    â‘¡ KitREC-Direct (Ablation) - No Thinking + Fine-tuned (Option A: ë³„ë„ í•™ìŠµ)
    â‘¢ Base-CoT (Strong Baseline) - Thinking + Untuned
    â‘£ Base-Direct (Weak Baseline) - No Thinking + Untuned
    """
    prompt_builder = PromptBuilder()

    models = {
        "kitrec_full": {
            "model": load_kitrec_model("kitrec-full"),  # ê¸°ì¡´ í•™ìŠµ ëª¨ë¸
            "prompt_fn": prompt_builder.build_thinking_prompt,
            "description": "Fine-tuned + Thinking"
        },
        "kitrec_direct": {
            # âš ï¸ Option A: <think> ì œê±°ëœ ë°ì´í„°ë¡œ ë³„ë„ í•™ìŠµí•œ ëª¨ë¸ ì‚¬ìš©
            "model": load_kitrec_model("kitrec-direct"),  # ë³„ë„ í•™ìŠµ í•„ìš”
            "prompt_fn": prompt_builder.build_direct_prompt,
            "description": "Fine-tuned (No Thinking) + Direct Output"
        },
        "base_cot": {
            "model": load_base_model(),  # Qwen3-14B-Instruct
            "prompt_fn": prompt_builder.build_thinking_prompt,
            "description": "Untuned + Thinking"
        },
        "base_direct": {
            "model": load_base_model(),  # Qwen3-14B-Instruct
            "prompt_fn": prompt_builder.build_direct_prompt,
            "description": "Untuned + No Thinking"
        }
    }

    for model_name, config in models.items():
        print(f"Evaluating {model_name}: {config['description']}")
        evaluate_model(config["model"], config["prompt_fn"])


# ============================================================================
# KitREC-Direct Option A: í•™ìŠµ ë°ì´í„° ì¤€ë¹„ (ë³„ë„ í•™ìŠµìš©)
# ============================================================================

def prepare_kitrec_direct_training_data(original_data_path: str, output_path: str):
    """
    KitREC-Direct í•™ìŠµìš© ë°ì´í„° ìƒì„± (Option A)
    - Training ë°ì´í„°ì˜ outputì—ì„œ <think>...</think> ë¸”ë¡ ì œê±°
    - JSON ì¶œë ¥ë§Œ ë‚¨ê¹€

    âš ï¸ ì´ í•¨ìˆ˜ë¡œ ìƒì„±ëœ ë°ì´í„°ë¡œ ë³„ë„ ëª¨ë¸ì„ í•™ìŠµí•´ì•¼ í•¨
    """
    import json

    def remove_thinking_block(output: str) -> str:
        """<think>...</think> ë¸”ë¡ ì œê±°"""
        pattern = r'<think>[\s\S]*?</think>\s*'
        return re.sub(pattern, '', output).strip()

    with open(original_data_path, 'r') as f:
        original_data = [json.loads(line) for line in f]

    direct_data = []
    for sample in original_data:
        direct_sample = {
            "instruction": sample["instruction"],
            "input": sample.get("input", ""),
            "output": remove_thinking_block(sample["output"]),  # <think> ì œê±°
            "metadata": sample.get("metadata", {})
        }
        direct_data.append(direct_sample)

    with open(output_path, 'w') as f:
        for sample in direct_data:
            f.write(json.dumps(sample, ensure_ascii=False) + '\n')

    print(f"KitREC-Direct training data saved: {output_path}")
    print(f"Original samples: {len(original_data)}")
    print(f"Direct samples: {len(direct_data)}")

# Usage:
# prepare_kitrec_direct_training_data(
#     "data/kitrec-dualft_music-seta/train.jsonl",
#     "data/kitrec-direct_music-seta/train.jsonl"
# )
```

### 5.3 `scripts/run_metadata_subgroup.py`

```python
"""
Movies Metadata ë¶„ë¦¬ í‰ê°€
âš ï¸ CLAUDE.md Sub-group Analysis: Group A/B ë¶„ë¦¬ í•„ìˆ˜
"""

from src.metrics.stratified_analysis import StratifiedAnalysis

def analyze_metadata_subgroups(results_path: str, metadata_path: str):
    """
    Movies ë„ë©”ì¸ 43.3% ë©”íƒ€ë°ì´í„° ëˆ„ë½ ë¬¸ì œ ë¶„ì„

    Group A: GT itemì— Title/Category ì¡´ì¬ â†’ KitREC ì‹¤ì œ ì„±ëŠ¥
    Group B: GT itemì´ Unknown â†’ ë©”íƒ€ë°ì´í„° ì˜ì¡´ë„ ì¸¡ì •

    ì˜ˆìƒ ê²°ê³¼: Group A ì„±ëŠ¥ >> Group B ì„±ëŠ¥
    """
    results = load_results(results_path)
    metadata_lookup = load_metadata_availability(metadata_path)

    analyzer = StratifiedAnalysis()
    subgroup_analysis = analyzer.analyze_by_metadata_availability(
        results,
        metadata_lookup
    )

    print("=== Movies Metadata Sub-group Analysis ===")
    print(f"Group A (with metadata): {subgroup_analysis['group_a_with_metadata']}")
    print(f"Group B (unknown): {subgroup_analysis['group_b_unknown']}")

    # ì„±ëŠ¥ ì°¨ì´ ê³„ì‚°
    diff = {
        metric: subgroup_analysis['group_a_with_metadata'].get(metric, 0) -
                subgroup_analysis['group_b_unknown'].get(metric, 0)
        for metric in ['hit@10', 'ndcg@10', 'mrr']
    }
    print(f"Performance Gap (A - B): {diff}")
```

---

## 6. ì‹¤í—˜ ë¡œë“œë§µ (Experiment Roadmap)

### Phase 1: í™˜ê²½ ì„¤ì • ë° ë°ì´í„° ì¤€ë¹„
| ìˆœì„œ | ì‘ì—… | ì™„ë£Œ ê¸°ì¤€ |
|------|------|----------|
| 1.1 | vLLM í™˜ê²½ ì„¤ì¹˜ | `python -c "import vllm"` ì„±ê³µ |
| 1.2 | GPU í™•ì¸ (Nvidia 5090, 36GB) | `nvidia-smi` ë©”ëª¨ë¦¬ í™•ì¸ |
| 1.3 | HuggingFace í† í° ì„¤ì • | Private ëª¨ë¸ ì ‘ê·¼ í…ŒìŠ¤íŠ¸ |
| 1.4 | Test Set ë¡œë”© ê²€ì¦ | 30,000 samples Ã— 2 sets í™•ì¸ |
| 1.5 | Candidate Set ë™ê¸°í™” ê²€ì¦ | 100ê°œ í›„ë³´ ì¼ì¹˜ í™•ì¸ |

### Phase 2: KitREC ëª¨ë¸ í‰ê°€
| ìˆœì„œ | ëª¨ë¸ | ìƒ˜í”Œ ìˆ˜ | ì˜ˆìƒ ì‹œê°„ |
|------|------|---------|----------|
| 2.1 | DualFT-Movies (Set A) | 15,000 | ~3h |
| 2.2 | DualFT-Movies (Set B) | 15,000 | ~3h |
| 2.3 | DualFT-Music (Set A) | 15,000 | ~3h |
| 2.4 | DualFT-Music (Set B) | 15,000 | ~3h |
| 2.5 | SingleFT-Movies (Set A) | 15,000 | ~3h |
| 2.6 | SingleFT-Movies (Set B) | 15,000 | ~3h |
| 2.7 | SingleFT-Music (Set A) | 15,000 | ~3h |
| 2.8 | SingleFT-Music (Set B) | 15,000 | ~3h |

### Phase 3: RQ1 Ablation Study (2Ã—2)
| ìˆœì„œ | ëª¨ë¸ | ì„¤ëª… |
|------|------|------|
| 3.1 | KitREC-Full | Phase 2 ê²°ê³¼ í™œìš© |
| 3.2 | KitREC-Direct | Thinking ì œê±° ë²„ì „ í‰ê°€ |
| 3.3 | Base-CoT | Zero-shot + Thinking |
| 3.4 | Base-Direct | Vanilla Zero-shot |

### Phase 4: Baseline ëª¨ë¸ í‰ê°€
| ìˆœì„œ | ëª¨ë¸ | ì‘ì—… ë‚´ìš© |
|------|------|----------|
| 4.1 | CoNet | í•™ìŠµ + ë™ì¼ Candidate Set í‰ê°€ |
| 4.2 | DTCDR | í•™ìŠµ + ë™ì¼ Candidate Set í‰ê°€ |
| 4.3 | LLM4CDR | 3-stage pipeline í‰ê°€ |
| 4.4 | Vanilla Zero-shot | Base-Directì™€ ë™ì¼ |

### Phase 5: Stratified Analysis
| ìˆœì„œ | ë¶„ì„ | ëª©ì  |
|------|------|------|
| 5.1 | User Typeë³„ (1~10 core) | RQ3: Cold-start ì„±ëŠ¥ |
| 5.2 | Movies Metadata Sub-group | Group A/B ë¶„ë¦¬ |
| 5.3 | Set A vs Set B | Hard vs Random ë¹„êµ |

### Phase 6: ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±
| ìˆœì„œ | ì‚°ì¶œë¬¼ | ë‚´ìš© |
|------|--------|------|
| 6.1 | rq1_ablation_report.md | 2Ã—2 Ablation ê²°ê³¼ |
| 6.2 | rq2_cdr_comparison.md | Baseline ë¹„êµ í…Œì´ë¸” |
| 6.3 | rq3_coldstart_analysis.md | Core Levelë³„ ê·¸ë˜í”„ |
| 6.4 | rq4_explainability.md | MAE/RMSE/PPL ê²°ê³¼ |
| 6.5 | final_paper_tables.md | ë…¼ë¬¸ìš© LaTeX í…Œì´ë¸” |

---

## 7. í•µì‹¬ ì²´í¬í¬ì¸íŠ¸ (CLAUDE.md ì°¸ì¡°)

| # | ì²´í¬í¬ì¸íŠ¸ | CLAUDE.md ì„¹ì…˜ | ê²€ì¦ ë°©ë²• |
|---|-----------|---------------|----------|
| CP1 | `input` í•„ë“œì—ì„œ í”„ë¡¬í”„íŠ¸ ì¶”ì¶œ | Critical Notes #1 | ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ |
| CP2 | Direct ëª¨ë¸ìš© Reasoning ì œê±° | RQ1 Note | ì¶œë ¥ì— `<think>` ì—†ìŒ í™•ì¸ |
| CP3 | Confidence Score Ã· 2 ì •ê·œí™” | Evaluation Metrics | MAE/RMSE ë²”ìœ„ ê²€ì¦ |
| CP4 | í›„ë³´êµ° ì™¸ item_id â†’ fail | Critical Notes #3 | ì˜¤ë¥˜ìœ¨ ë¡œê¹… |
| CP5 | ëª¨ë“  Baseline ë™ì¼ Candidate Set | Baseline ê³µì •ì„± | ID ë¦¬ìŠ¤íŠ¸ ë¹„êµ |
| CP6 | Baseline History ë™ì¼ ì‹œì  | Critical Notes #4 | ë°ì´í„° ê²€ì¦ |
| CP7 | Movies Group A/B ë¶„ë¦¬ | Sub-group Analysis | ë¶„ë¦¬ ì¹´ìš´íŠ¸ í™•ì¸ |

---

## 8. ì˜ˆìƒ ì‚°ì¶œë¬¼

```
results/
â”œâ”€â”€ kitrec/
â”‚   â””â”€â”€ {model}_{set}/
â”‚       â”œâ”€â”€ predictions.jsonl          # ì „ì²´ ì˜ˆì¸¡ ê²°ê³¼
â”‚       â”œâ”€â”€ metrics_summary.json       # ì§‘ê³„ ë©”íŠ¸ë¦­
â”‚       â””â”€â”€ error_statistics.json      # íŒŒì‹± ì˜¤ë¥˜ í†µê³„
â”‚
â”œâ”€â”€ ablation/
â”‚   â””â”€â”€ comparison_table.md            # 2Ã—2 ë¹„êµ í…Œì´ë¸”
â”‚
â”œâ”€â”€ baselines/
â”‚   â””â”€â”€ {model}/
â”‚       â”œâ”€â”€ predictions.jsonl
â”‚       â””â”€â”€ metrics_summary.json
â”‚
â””â”€â”€ reports/
    â”œâ”€â”€ rq1_ablation_report.md
    â”œâ”€â”€ rq2_cdr_comparison.md
    â”œâ”€â”€ rq3_coldstart_analysis.md
    â”œâ”€â”€ rq4_explainability.md
    â””â”€â”€ final_paper_tables.md           # LaTeX í…Œì´ë¸” í¬í•¨
```

---

## 9. êµ¬í˜„ ìƒì„¸ (Implementation Details)

### 9.1 Baseline ê³µí†µ ì¸í”„ë¼ (`baselines/base_evaluator.py`)

```python
"""
ëª¨ë“  ë² ì´ìŠ¤ë¼ì¸ì´ ê³µìœ í•˜ëŠ” ê³µí†µ í‰ê°€ ì¸í”„ë¼

ì£¼ìš” ê¸°ëŠ¥:
1. Candidate Set ê²€ì¦ (100ê°œ + GT í¬í•¨)
2. Confidence Score ì •ê·œí™” ([0,1] â†’ [1,10])
3. ê³µí†µ ë©”íŠ¸ë¦­ ê³„ì‚°
"""

class BaseEvaluator(ABC):
    def __init__(self, device: str = "cuda"):
        self.device = device if torch.cuda.is_available() else "cpu"
        self.metrics = RankingMetrics()

    def validate_candidate_set(
        self,
        candidates: list,
        gt_id: Optional[int] = None,
        raise_on_error: bool = True
    ) -> bool:
        """
        Candidate Set ê²€ì¦
        - 100ê°œ í›„ë³´ í™•ì¸
        - GT í¬í•¨ ì—¬ë¶€ í™•ì¸
        """
        if len(candidates) != 100:
            msg = f"Expected 100 candidates, got {len(candidates)}"
            if raise_on_error:
                raise ValueError(msg)
            print(f"Warning: {msg}")
            return False

        if gt_id is not None and gt_id not in candidates:
            msg = f"Ground truth {gt_id} not in candidate set"
            if raise_on_error:
                raise ValueError(msg)
            print(f"Warning: {msg}")
            return False

        return True

    def normalize_confidence(self, raw_score: float) -> float:
        """
        Confidence Score ì •ê·œí™”

        KitREC ë²”ìœ„: 1-10
        Baseline ì¶œë ¥: sigmoid(raw_score) âˆˆ [0,1]
        ë³€í™˜: sigmoid * 9 + 1 â†’ [1,10]
        """
        sigmoid = 1 / (1 + np.exp(-raw_score))
        return sigmoid * 9 + 1
```

### 9.2 User Type Mapping (RQ3 Cold-start Analysis)

```python
"""
User Typeë³„ ë¶„ì„ì„ ìœ„í•œ ë§¤í•‘ êµ¬ì¶•

baselines/dtcdr/evaluator.pyì— ì¶”ê°€ëœ ë©”ì„œë“œ:
"""

def evaluate_by_user_type(
    self,
    samples: List[DTCDRSample],
    user_type_mapping: Dict[int, str]
) -> Dict[str, Dict[str, float]]:
    """
    User Typeë³„ í‰ê°€ (RQ3: Cold-start analysis)

    Args:
        samples: í‰ê°€ ìƒ˜í”Œ ë¦¬ìŠ¤íŠ¸
        user_type_mapping: {user_id: user_type}

    Returns:
        {user_type: {metric: value}}
    """
    from collections import defaultdict

    grouped = defaultdict(list)

    for sample in samples:
        user_type = user_type_mapping.get(sample.user_id, "unknown")
        metrics = self.evaluate_sample(sample)
        grouped[user_type].append(metrics)

    results = {}
    for user_type, metrics_list in grouped.items():
        aggregated = {}
        for key in metrics_list[0].keys():
            if key != "rank":
                values = [m[key] for m in metrics_list]
                aggregated[key] = np.mean(values)
        aggregated["sample_count"] = len(metrics_list)
        results[user_type] = aggregated

    return results
```

### 9.3 GPT-4.1 Rationale Evaluation (RQ4)

> âš ï¸ **RQ4ëŠ” KitREC ëª¨ë¸ë§Œ í‰ê°€ ëŒ€ìƒ** (Baseline ì œì™¸)

```python
"""
GPT-4.1 APIë¥¼ í†µí•œ Rationale í’ˆì§ˆ í‰ê°€

src/metrics/explainability_metrics.pyì˜ GPTRationaleEvaluator í´ë˜ìŠ¤:
"""

class GPTRationaleEvaluator:
    """
    User Typeë³„ ê· ë“± ì¶”ì¶œ(Stratified Sampling)ë¡œ ëª¨ë¸ë‹¹ 50ê°œ ìƒ˜í”Œ í‰ê°€
    - 10ê°œ User Type Ã— 5ê°œ/Type = 50ê°œ/ëª¨ë¸
    - ë¹„ìš© íš¨ìœ¨ì ì´ë©´ì„œ User Typeë³„ ê· í˜• ì¡íŒ í‰ê°€

    í‰ê°€ ê¸°ì¤€ (1-10ì ):
    1. ë…¼ë¦¬ì„± (logic): ì¶”ì²œ ì´ìœ ê°€ ë…¼ë¦¬ì ì¸ê°€?
    2. êµ¬ì²´ì„± (specificity): êµ¬ì²´ì ì¸ ê·¼ê±°ë¥¼ ì œì‹œí•˜ëŠ”ê°€?
    3. Cross-domain ì—°ê²°ì„± (cross_domain): Sourceâ†’Target ì—°ê²°ì´ ëª…í™•í•œê°€?
    4. ì‚¬ìš©ì ì„ í˜¸ ë°˜ì˜ (preference): íˆìŠ¤í† ë¦¬ë¥¼ ì˜ ë°˜ì˜í–ˆëŠ”ê°€?
    """

    EVALUATION_PROMPT = '''You are an expert evaluator...
    Respond ONLY with a JSON object:
    {"logic": <1-10>, "specificity": <1-10>, "cross_domain": <1-10>,
     "preference": <1-10>, "overall": <1-10>}'''

    def __init__(
        self,
        api_key: Optional[str] = None,
        samples_per_type: int = 5,  # ê° User Typeë‹¹ 5ê°œ (ì´ 50ê°œ/ëª¨ë¸)
        model: str = "gpt-4.1",
        random_seed: int = 42
    ):
        self.samples_per_type = samples_per_type
        self.model = model
        # OpenAI client ì´ˆê¸°í™”...

    def evaluate_batch(self, results: List[Dict]) -> Dict[str, float]:
        """
        User Typeë³„ ê· ë“± ì¶”ì¶œ (Stratified Sampling) í›„ GPT-4.1 í‰ê°€

        Returns:
            {
                "logic": mean_score,
                "specificity": mean_score,
                "cross_domain": mean_score,
                "preference": mean_score,
                "overall": mean_score,
                "n_evaluated": int,
                "n_total": int,
                "sampling_stats": {user_type: {total, sampled}}
            }
        """
```

### 9.4 Statistical Significance Testing

```python
"""
í†µê³„ì  ìœ ì˜ì„± ê²€ì • (src/metrics/statistical_analysis.py)

ì£¼ìš” ê¸°ëŠ¥:
1. Paired t-test (RQ1, RQ2)
2. Multiple comparison correction (Holm-Bonferroni)
3. Bootstrap confidence intervals
4. ë…¼ë¬¸ìš© í˜•ì‹ ì¶œë ¥
"""

class StatisticalAnalysis:
    @staticmethod
    def paired_t_test(scores_a: List[float], scores_b: List[float]) -> Dict:
        """Paired t-test with Cohen's d effect size"""
        t_stat, p_value = stats.ttest_rel(scores_a, scores_b)
        diff = np.array(scores_a) - np.array(scores_b)
        effect_size = np.mean(diff) / np.std(diff) if np.std(diff) > 0 else 0

        return {
            "t_statistic": float(t_stat),
            "p_value": float(p_value),
            "significant_at_0.05": p_value < 0.05,
            "significant_at_0.01": p_value < 0.01,
            "significant_at_0.001": p_value < 0.001,
            "effect_size_cohens_d": float(effect_size),
        }

    @staticmethod
    def apply_multiple_correction(
        p_values: List[float],
        method: str = "holm"
    ) -> Dict:
        """
        ë‹¤ì¤‘ ë¹„êµ ë³´ì • (Holm-Bonferroni)

        Step-up enforcementë¡œ adjusted p-value ê³„ì‚°
        """
        n = len(p_values)
        sorted_indices = np.argsort(p_values)
        sorted_p = np.array(p_values)[sorted_indices]

        adjusted_p = np.zeros(n)
        significant = np.zeros(n, dtype=bool)

        # Step-up enforcement
        max_adjusted = 0.0
        for i, idx in enumerate(sorted_indices):
            raw_adjusted = sorted_p[i] * (n - i)
            max_adjusted = max(max_adjusted, raw_adjusted)
            adjusted_p[idx] = min(max_adjusted, 1.0)

        return {
            "adjusted_p_values": adjusted_p.tolist(),
            "significant": significant.tolist()
        }
```

### 9.5 LLM4CDR êµ¬í˜„ ì°¨ì´ì  ë¬¸ì„œí™”

```python
"""
LLM4CDR ì› ë…¼ë¬¸ vs KitREC êµ¬í˜„ ì°¨ì´ì 
(baselines/llm4cdr/prompts.pyì— ë¬¸ì„œí™”)
"""

LLM4CDR_IMPLEMENTATION_NOTES = """
## KitREC vs Original LLM4CDR Implementation

### Key Differences:

1. **Candidate Set Size**:
   - Original: ~30 items (3 GT + 20-30 negatives)
   - KitREC: 100 items (1 GT + 99 negatives)

2. **Target History**:
   - Original: Not used
   - KitREC: Included for fair comparison

3. **Stage 1 Caching**:
   - Both: Domain gap analysis is cached per domain pair

### Paper Citation Note:
When citing results, note that LLM4CDR was re-evaluated
using KitREC's more challenging evaluation protocol.
"""
```

---

## 10. ë…¼ë¬¸ ì‘ì„±ìš© Quick Reference

### 10.1 RQë³„ ë©”íŠ¸ë¦­ ë§¤í•‘

| RQ | ì—°êµ¬ ì§ˆë¬¸ | ì£¼ìš” ë©”íŠ¸ë¦­ | í†µê³„ ê²€ì • |
|----|---------|-----------|----------|
| RQ1 | Ablation Study | Hit@10, NDCG@10 | Paired t-test |
| RQ2 | Baseline ë¹„êµ | Hit@1/5/10, MRR, NDCG@5/10 | Paired t-test + Holm |
| RQ3 | Cold-start | Core Levelë³„ Hit@10 | User Type ë¶„ë¦¬ |
| RQ4 | Explainability (**KitRECë§Œ**) | MAE, RMSE, GPT Score | Stratified 50ê°œ/ëª¨ë¸ |

### 10.2 ë…¼ë¬¸ í…Œì´ë¸” í˜•ì‹ ì˜ˆì‹œ

```latex
% RQ2: Baseline Comparison Table
\begin{table}[h]
\centering
\caption{Performance comparison on KitREC test set}
\begin{tabular}{lcccccc}
\toprule
Model & Hit@1 & Hit@5 & Hit@10 & MRR & NDCG@5 & NDCG@10 \\
\midrule
CoNet & 0.xxx & 0.xxx & 0.xxx & 0.xxx & 0.xxx & 0.xxx \\
DTCDR & 0.xxx & 0.xxx & 0.xxx & 0.xxx & 0.xxx & 0.xxx \\
LLM4CDR & 0.xxx & 0.xxx & 0.xxx & 0.xxx & 0.xxx & 0.xxx \\
\midrule
KitREC & \textbf{0.xxx} & \textbf{0.xxx}** & \textbf{0.xxx}*** & ... \\
\bottomrule
\multicolumn{7}{l}{\footnotesize * p<0.05, ** p<0.01, *** p<0.001 (paired t-test)} \\
\end{tabular}
\end{table}
```

### 10.3 í•µì‹¬ ì¸ìš© ë¬¸êµ¬

```
# Baseline ë¹„êµ ë°©ë²•ë¡ 
"All baseline models were evaluated on the identical candidate set
(1 ground truth + 99 negatives) to ensure fair comparison."

# LLM4CDR ì¬í‰ê°€
"LLM4CDR was re-implemented and evaluated using our evaluation
protocol for fair comparison. The original paper uses a smaller
candidate set (3 GT + 20-30 negatives)."

# Cold-start ë¶„ì„
"Unlike existing CDR methods that only evaluate on users with
5+ target interactions, we specifically analyze performance on
extreme cold-start users (1-4 target interactions)."

# GPT-4.1 í‰ê°€
"Rationale quality was evaluated on a 10% random sample using
GPT-4.1 API, scoring logic, specificity, cross-domain connection,
and preference alignment on a 1-10 scale."
```

---

**ì‘ì„± ì™„ë£Œ: 2025-12-06**
**ìµœì¢… ìˆ˜ì •: 2025-12-07 (êµ¬í˜„ ì™„ë£Œ ë°˜ì˜)**
